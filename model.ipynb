{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To do:\n",
        "- Remove dropping NAs from user_similarity_df in fetch_similar_users()"
      ],
      "metadata": {
        "id": "T7DZ0Cwb3-F4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSkyK2IsuqNl"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnZAe-CVuqNp",
        "outputId": "c917fbf3-22e4-4e25-d661-6d8833460944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse import load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import heapq\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select multiple user IDs & timestamps for evaluation\n",
        "def select_user_ids_timestamps(k=5):\n",
        "  # Select the top 10 rows\n",
        "  filtered_behaviors_df = behaviors_df.tail(k)\n",
        "\n",
        "  # Create a list of tuples containing values from columns 'a' and 'b'\n",
        "  user_ids_timestamps = [(row['User ID'], row['Timestamp']) for _, row in filtered_behaviors_df.iterrows()]\n",
        "\n",
        "  return user_ids_timestamps"
      ],
      "metadata": {
        "id": "7mrNpiOtNvU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85PgkxPTuqNr"
      },
      "source": [
        "## Import Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Ww33DFuqNr",
        "outputId": "c9e22fde-38bf-4de3-9cdd-e167313a25df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "news_df = pd.read_pickle(\"/content/news.csv\")\n",
        "\n",
        ")\n",
        "\n",
        "# Load pre-trained Google News Word2Vec model\n",
        "model_path = \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin\"\n",
        "google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "# import behaviors df\n",
        "behaviors_df = pd.read_csv(\"/content/behaviors.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "## Install Necessary Libraries\n",
        "!pip install wordninja --quiet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import ast\n",
        "import wordninja\n",
        "import warnings\n",
        "from scipy import sparse\n",
        "from scipy.sparse import csc_matrix, lil_matrix, save_npz\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "# Choose Dataset Characteristics\n",
        "size = 'Small'\n",
        "version = 'Train'\n",
        "\n",
        "## Clean News Dataset\n",
        "\n",
        "# Load & Visualize Input Datasets\n",
        "news = pd.read_csv(f\"/content/news.csv\",\n",
        "                   header=None, sep='\\t',\n",
        "                   names=['News ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entities'],\n",
        "                   usecols=['News ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'Title Entities', 'Abstract Entities'])\n",
        "\n",
        "# Transformations\n",
        "def get_clean_news(news, embeddings):\n",
        "    # Remove rows with missing values\n",
        "    news.dropna(inplace=True)\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    news.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Split and replace words\n",
        "    def split_and_replace(word):\n",
        "        split_words = wordninja.split(word)\n",
        "        new_word = ' '.join(split_words)\n",
        "        return new_word\n",
        "\n",
        "    news['Category'] = news['Category'].apply(lambda x: split_and_replace(x))\n",
        "    news['SubCategory'] = news['SubCategory'].apply(lambda x: split_and_replace(x))\n",
        "\n",
        "    # Convert selected columns to lowercase\n",
        "    columns_to_lower = ['Category', 'SubCategory', 'Title', 'Abstract']\n",
        "    news[columns_to_lower] = news[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "    # Create content column\n",
        "    news['Content'] = news[['Category', 'SubCategory', 'Title', 'Abstract']].apply(' '.join, axis=1)\n",
        "\n",
        "    # Remove punctuation and stopwords from content\n",
        "    news['Content'] = news['Content'].str.replace('[{}]'.format(string.punctuation), '', regex=True)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    news['Content'] = [' '.join(word for word in content.split() if word.lower() not in stop_words) for content in news['Content']]\n",
        "\n",
        "    # Calculate word count\n",
        "    news['Content_WC'] = news['Content'].str.split().str.len()\n",
        "\n",
        "    # Process embeddings for title and abstract\n",
        "    news['Title Entities'] = news['Title Entities'].apply(ast.literal_eval)\n",
        "    news['Abstract Entities'] = news['Abstract Entities'].apply(ast.literal_eval)\n",
        "    news['Title Wikidata IDs'] = news['Title Entities'].apply(lambda x: ' '.join([d['WikidataId'] for d in x]))\n",
        "    news['Abstract Wikidata IDs'] = news['Abstract Entities'].apply(lambda x: ' '.join([d['WikidataId'] for d in x]))\n",
        "    news['All Wikidata IDs'] = news['Title Wikidata IDs'] + ' ' + news['Abstract Wikidata IDs']\n",
        "\n",
        "    def calculate_average_vector(vector_ids, embeddings):\n",
        "        vectors = [embeddings.get(vector_id) for vector_id in vector_ids if vector_id in embeddings]\n",
        "        return np.mean(vectors, axis=0) if vectors else np.nan\n",
        "\n",
        "    news['Average Vector'] = news['All Wikidata IDs'].apply(lambda x: calculate_average_vector(x.split(), embeddings))\n",
        "\n",
        "    # Impute missing values\n",
        "    subset = news.dropna(subset=['Average Vector'])\n",
        "    avg_vector_cat_subcat = subset.groupby(['Category', 'SubCategory'])['Average Vector'].mean().to_dict()\n",
        "    news['Average Vector'] = news.apply(\n",
        "        lambda row: avg_vector_cat_subcat.get((row['Category'], row['SubCategory']))\n",
        "        if pd.isna(row['Average Vector']) else row['Average Vector'], axis=1)\n",
        "\n",
        "    avg_vector_cat = subset.groupby('Category')['Average Vector'].mean().to_dict()\n",
        "    news['Average Vector'] = news.apply(\n",
        "        lambda row: avg_vector_cat.get(row['Category'])\n",
        "        if pd.isna(row['Average Vector']) else row['Average Vector'], axis=1)\n",
        "\n",
        "    news.drop(columns=['Title Entities', 'Abstract Entities', 'Title Wikidata IDs', 'Abstract Wikidata IDs', 'All Wikidata IDs'], inplace=True)\n",
        "    return news\n",
        "\n",
        "news_df = get_clean_news(news, embeddings)\n",
        "\n",
        "\n",
        "## Clean Behaviors Dataset\n",
        "\n",
        "# Load and visualize input dataset\n",
        "behaviors = pd.read_csv(\"/content/behaviors.tsv\",\n",
        "                        header=None, sep='\\t',\n",
        "                        names=['Impression ID', 'User ID', 'Timestamp', 'History', 'Impressions'],\n",
        "                        usecols=['User ID', 'Timestamp', 'History', 'Impressions'])\n",
        "\n",
        "# Transformations\n",
        "def get_clean_behaviors(behaviors, news):\n",
        "    behaviors['Timestamp'] = pd.to_datetime(behaviors['Timestamp'])\n",
        "    behaviors['Impressions'] = behaviors['Impressions'].str.replace(r'\\w+-0', '', regex=True).str.strip().str.replace('-1', '')\n",
        "    behaviors['History'] = behaviors['History'].str.strip().astype(str)\n",
        "    behaviors['History'] = behaviors['History'].apply(lambda x: ' '.join(list(set(x.split()))))\n",
        "    behaviors.drop_duplicates(inplace=True)\n",
        "\n",
        "    valid_news_ids = set(news['News ID'])\n",
        "    behaviors['History'] = behaviors['History'].apply(lambda x: ' '.join(news_id for news_id in x.split() if news_id in valid_news_ids))\n",
        "    behaviors['Impressions'] = behaviors['Impressions'].apply(lambda x: ' '.join(news_id for news_id in x.split() if news_id in valid_news_ids))\n",
        "\n",
        "    behaviors['History & Impressions'] = behaviors['History'] + ' ' + behaviors['Impressions']\n",
        "    user_articles = behaviors['History & Impressions'].str.split().apply(set)\n",
        "    avg_vectors_dict = news.groupby('News ID')['Average Vector'].mean().to_dict()\n",
        "\n",
        "    def calculate_average_vector(article_ids):\n",
        "        vectors = [avg_vectors_dict.get(article_id) for article_id in article_ids if article_id in avg_vectors_dict]\n",
        "        return np.mean(vectors, axis=0) if vectors else None\n",
        "\n",
        "    behaviors['Average Vector'] = user_articles.apply(calculate_average_vector)\n",
        "    return behaviors\n",
        "\n",
        "behaviors_df = get_clean_behaviors(behaviors, news)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CEOYFQvV4aqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative + Content Model"
      ],
      "metadata": {
        "id": "hxB0QzDoiQKo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CKAv7qduqNt"
      },
      "source": [
        "## Define Functions for User to User Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps):\n",
        "    # Extract rows corresponding to similar users and their respective timestamps\n",
        "    similar_users_df = behaviors_df[\n",
        "        behaviors_df[['User ID', 'Timestamp']].apply(tuple, axis=1).isin(similar_users_timestamps)\n",
        "    ]\n",
        "\n",
        "    # Initialize a list to collect article IDs for recommendations\n",
        "    recommended_article_ids = []\n",
        "\n",
        "    # Collect articles from the interaction history of similar users\n",
        "    for _, row in similar_users_df.iterrows():\n",
        "        recommended_article_ids.extend(row['History & Impressions'].split())\n",
        "\n",
        "    # Retrieve the list of articles the target user has already interacted with\n",
        "    previously_read_article_ids = (\n",
        "        list(\n",
        "            behaviors_df.loc[\n",
        "                ((behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp)),\n",
        "                'History'\n",
        "            ].str.split()\n",
        "        )[0]\n",
        "    )\n",
        "\n",
        "    # Exclude articles already interacted with by the user from the recommendations\n",
        "    recommended_article_ids = list(\n",
        "        set([article_id for article_id in recommended_article_ids if article_id not in previously_read_article_ids])\n",
        "    )\n",
        "\n",
        "    return recommended_article_ids\n"
      ],
      "metadata": {
        "id": "LfVNjJrm3Bmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpCRATmLuqNu"
      },
      "source": [
        "## Define Functions for Content Based Filtering - Post Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "2CTye4a4O431"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation: `create_previously_read_content`\n",
        "\n",
        "The `create_previously_read_content` function extracts and returns a list of words from all articles previously read by a specific user. Below is a detailed explanation of the function:\n",
        "\n",
        "---\n",
        "\n",
        "#### **Purpose**\n",
        "This function is designed to collect content from articles a user has read. The aggregated content can later be used in similarity-based recommendation systems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Inputs**\n",
        "- **`user_id`**: A string representing the unique identifier of the target user.\n",
        "- **`timestamp`**: A string representing the specific interaction time of the user.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Outputs**\n",
        "- **`previously_read_content`**: A list of all words extracted from the content of articles the user has read.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Steps**\n",
        "1. **Retrieve Previously Read Articles**:\n",
        "   - The function filters the `behaviors_df` dataframe to locate the row corresponding to the specified `user_id` and `timestamp`.\n",
        "   - From the `History` column, it extracts a list of article IDs representing the user's previously read articles.\n",
        "\n",
        "2. **Filter News Content**:\n",
        "   - Using the list of previously read article IDs, the function filters the `news_df` dataframe.\n",
        "   - Only the rows with matching article IDs are selected, and their `Content` is retrieved.\n",
        "\n",
        "3. **Aggregate Words**:\n",
        "   - The content of all relevant articles is concatenated into a single string.\n",
        "   - The string is then split into individual words to create a list of all words from the articles.\n",
        "\n",
        "4. **Return Content**:\n",
        "   - The final list of words is returned for use in content-based recommendation systems.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example Usage**\n",
        "```python\n",
        "# Example\n",
        "previously_read_words = create_previously_read_content(user_id='U13740', timestamp='2019-11-13 15:27:40')\n",
        "print(previously_read_words)\n"
      ],
      "metadata": {
        "id": "24pvq6S7FQFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_previously_read_content(user_id, timestamp):\n",
        "    \"\"\"\n",
        "    Extracts all content words from articles previously read by a given user.\n",
        "\n",
        "    Parameters:\n",
        "    - user_id: ID of the target user.\n",
        "    - timestamp: Specific timestamp of the user's interaction.\n",
        "\n",
        "    Returns:\n",
        "    - previously_read_content: List of words from all articles the user has previously read.\n",
        "    \"\"\"\n",
        "    # Retrieve the IDs of articles in the user's interaction history\n",
        "    previously_read_article_ids = (\n",
        "        list(\n",
        "            behaviors_df.loc[\n",
        "                (behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp),\n",
        "                'History'\n",
        "            ].str.split()\n",
        "        )[0]\n",
        "    )\n",
        "\n",
        "    # Filter the news dataframe to include only articles the user has interacted with\n",
        "    previously_read_articles_df = news_df[\n",
        "        news_df['News ID'].isin(previously_read_article_ids)\n",
        "    ][['News ID', 'Content']]\n",
        "\n",
        "    # Combine the content of all previously read articles and split into individual words\n",
        "    previously_read_content = ' '.join(previously_read_articles_df['Content']).split()\n",
        "\n",
        "    return previously_read_content\n"
      ],
      "metadata": {
        "id": "Gue3O2a-OVw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the `create_recommended_content` Function\n",
        "\n",
        "## **Purpose**\n",
        "The function generates a dictionary where each key is a `News ID` from the recommended articles, and the value is a list of words in the content of the respective article.\n",
        "\n",
        "---\n",
        "\n",
        "## **Inputs**\n",
        "1. **`recommended_article_ids`**:  \n",
        "   A list of `News ID`s recommended by the collaborative filtering algorithm.\n",
        "\n",
        "2. **`news_df`**:  \n",
        "   The cleaned news dataset, which contains columns like `News ID` and `Content`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Output**\n",
        "- **`recommended_content`**:  \n",
        "  A dictionary with:\n",
        "  - **Keys**: `News ID`s of the recommended articles.\n",
        "  - **Values**: Lists of words present in the article content.\n",
        "\n",
        "---\n",
        "\n",
        "## **Steps in the Function**\n",
        "1. **Filter the DataFrame**:\n",
        "   - Use `query()` to filter `news_df` based on the `recommended_article_ids`.\n",
        "   - Select only the columns `News ID` and `Content` for further processing.\n",
        "\n",
        "   ```python\n",
        "   filtered_df = news_df.query(\"`News ID` in @recommended_article_ids\")[['News ID', 'Content']]\n"
      ],
      "metadata": {
        "id": "zaTWNDjfOZCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_recommended_content(recommended_article_ids):\n",
        "    '''\n",
        "    Inputs:\n",
        "    recommended_article_ids: List of article IDs recommended by the recommend_articles_collaborative function\n",
        "    news_df: Cleaned news DataFrame imported from drive\n",
        "\n",
        "    Outputs:\n",
        "    recommended_content: Dictionary with recommended article IDs as keys and list of words in article content as values\n",
        "    '''\n",
        "    # Filter the news DataFrame for the recommended article IDs\n",
        "    filtered_df = news_df.query(\"`News ID` in @recommended_article_ids\")[['News ID', 'Content']]\n",
        "\n",
        "    # Create the dictionary using a dictionary comprehension\n",
        "    recommended_content = {\n",
        "        row['News ID']: row['Content'].split()\n",
        "        for _, row in filtered_df.iterrows()\n",
        "    }\n",
        "\n",
        "    return recommended_content\n"
      ],
      "metadata": {
        "id": "ffN4kMLqQQZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Overview**\n",
        "This function recommends articles to a user based on the similarity of their content to articles the user has previously read. It uses Word2Vec embeddings to calculate cosine similarity between the word vectors of the previously read content and the content of recommended articles.\n",
        "\n",
        "---\n",
        "\n",
        "## **Inputs**\n",
        "1. **`previously_read_content`**:  \n",
        "   - A list of words representing the content of articles the user has already read.\n",
        "\n",
        "2. **`recommended_content`**:  \n",
        "   - A dictionary where:\n",
        "     - **Keys**: Article IDs of recommended articles.\n",
        "     - **Values**: Lists of words representing the content of those articles.\n",
        "\n",
        "3. **`k`** (optional):  \n",
        "   - Number of top recommendations to return.  \n",
        "   - Default value: `5`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Output**\n",
        "- **`final_recommended_article_ids`**:  \n",
        "   A list of the top `k` recommended article IDs based on cosine similarity.\n",
        "\n",
        "---\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "### **1. Filter Words Using Word2Vec Vocabulary**\n",
        "   - Ensure that only words present in the Word2Vec vocabulary are used.  \n",
        "   - This prevents errors when calculating similarities with words not known by the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Compute Similarity Scores**\n",
        "   - Use the Word2Vec model's `n_similarity` function to calculate cosine similarity between:\n",
        "     - **Previously read content**.\n",
        "     - **Each recommended article’s content**.\n",
        "   - Store the results in a dictionary where:\n",
        "     - **Keys**: Article IDs.\n",
        "     - **Values**: Similarity scores.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Retrieve Top `k` Articles**\n",
        "   - Use `heapq.nlargest` to efficiently select the top `k` article IDs with the highest similarity scores.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Return Recommendations**\n",
        "   - Return the `final_recommended_article_ids` list as the final output.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lcdyX5BAPp-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recommend articles based on word2vec similarity\n",
        "def recommend_articles_content_w2v(previously_read_content, recommended_content, k=5):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    previously_read_content: List of words in the content of articles the user has previously read.\n",
        "    recommended_content: Dictionary where keys are article IDs and values are lists of words in article content.\n",
        "\n",
        "    Output:\n",
        "    final_recommended_article_ids: List of the top `k` article IDs with the highest similarity scores.\n",
        "    \"\"\"\n",
        "    # Filter words present in the word2vec model vocabulary\n",
        "    filtered_user_content = [word for word in previously_read_content if word in google_model.key_to_index]\n",
        "    filtered_recommended_content = {\n",
        "        article_id: [word for word in content if word in google_model.key_to_index]\n",
        "        for article_id, content in recommended_content.items()\n",
        "    }\n",
        "\n",
        "    # Compute similarity scores\n",
        "    similarity_scores = {\n",
        "        article_id: google_model.n_similarity(filtered_user_content, content)\n",
        "        for article_id, content in filtered_recommended_content.items()\n",
        "    }\n",
        "\n",
        "    # Retrieve the top `k` articles with the highest similarity scores\n",
        "    final_recommended_article_ids = heapq.nlargest(k, similarity_scores, key=similarity_scores.get)\n",
        "\n",
        "    return final_recommended_article_ids\n"
      ],
      "metadata": {
        "id": "PSxqGiURQ4-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "iGe6OANaSNJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of `get_top_k_recommended_article_ids_avgvec`\n",
        "\n",
        "This function identifies the top-k recommended articles for a user by computing the cosine similarity between the average vector of the articles the user has previously read and the vectors of recommended articles. Here's a step-by-step explanation:\n",
        "\n",
        "---\n",
        "\n",
        "#### Inputs:\n",
        "- **`user_id`**: The ID of the user for whom the recommendations are being generated.\n",
        "- **`timestamp`**: The timestamp of the user's current interaction.\n",
        "- **`recommended_article_ids`**: A list of article IDs obtained from collaborative filtering recommendations.\n",
        "- **`k`**: The number of top recommendations to return.\n",
        "\n",
        "---\n",
        "\n",
        "#### Steps:\n",
        "1. **Filter News DataFrame for Recommended Articles**:\n",
        "   - Extracts only the rows in `news_df` that correspond to the `recommended_article_ids`.\n",
        "\n",
        "2. **Fetch User's Previously Read Article IDs**:\n",
        "   - Retrieves the history of articles read by the user based on their `user_id` and `timestamp`.\n",
        "\n",
        "3. **Calculate Average Vector for Previously Read Articles**:\n",
        "   - Combines the vectors of all previously read articles into a single \"average vector\" that represents the user's interests.\n",
        "\n",
        "4. **Exclude Previously Read Articles**:\n",
        "   - Filters out articles from the `filtered_news_df` that the user has already read to avoid recommending duplicates.\n",
        "\n",
        "5. **Compute Cosine Similarity**:\n",
        "   - Calculates the similarity between the user's average vector and the vectors of unread articles in `filtered_news_df`.\n",
        "\n",
        "6. **Sort Articles by Similarity**:\n",
        "   - Ranks the articles in descending order of similarity scores.\n",
        "\n",
        "7. **Select Top-k Articles**:\n",
        "   - Extracts the IDs of the top `k` articles with the highest similarity scores.\n",
        "\n",
        "8. **Handle Edge Cases**:\n",
        "   - Ensures the function can handle scenarios like empty user history or an empty list of recommendations.\n",
        "\n",
        "---\n",
        "\n",
        "#### Outputs:\n",
        "- **`top_k_recommended_article_ids`**:\n",
        "  A list containing the IDs of the top `k` recommended articles based on similarity.\n",
        "\n",
        "---\n",
        "\n",
        "This function ensures personalized recommendations by leveraging collaborative filtering results and refining them with content-based similarity using article vectors.\n"
      ],
      "metadata": {
        "id": "LLLz5theSGZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_recommended_article_ids_avgvec(user_id, timestamp, recommended_article_ids, k=5):\n",
        "    # Filter news_df to include only recommended articles from collaborative filtering\n",
        "    filtered_news_df = news_df[news_df['News ID'].isin(recommended_article_ids)].copy()\n",
        "\n",
        "    # Extract user's previously read article IDs\n",
        "    user_history_row = behaviors_df.loc[\n",
        "        (behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp),\n",
        "        'History'\n",
        "    ]\n",
        "    previously_read_article_ids = user_history_row.iloc[0].split() if not user_history_row.empty else []\n",
        "\n",
        "    # Calculate the average vector of previously read articles\n",
        "    previous_articles_df = news_df[news_df['News ID'].isin(previously_read_article_ids)]\n",
        "    average_news_vector = previous_articles_df['Average Vector'].mean()\n",
        "\n",
        "    # Exclude articles the user has already read\n",
        "    filtered_news_df = filtered_news_df[~filtered_news_df['News ID'].isin(previously_read_article_ids)]\n",
        "\n",
        "    # Calculate similarity scores\n",
        "    filtered_news_df['Similarity'] = filtered_news_df['Average Vector'].apply(\n",
        "        lambda x: cosine_similarity([average_news_vector], [x])[0][0] if average_news_vector is not None else 0\n",
        "    )\n",
        "\n",
        "    # Select top-k articles by similarity\n",
        "    top_k_recommended_article_ids = (\n",
        "        filtered_news_df.nlargest(k, 'Similarity')['News ID'].tolist()\n",
        "        if not filtered_news_df.empty else []\n",
        "    )\n",
        "\n",
        "    return top_k_recommended_article_ids\n"
      ],
      "metadata": {
        "id": "CaBGvL5pKTk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFRePjiQXuvB"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of `recommend_articles_content_tfidf`\n",
        "\n",
        "This function refines the list of recommended articles by utilizing TF-IDF-based content similarity. It calculates the similarity between the TF-IDF features of articles read by a user and the articles recommended by collaborative filtering.\n",
        "\n",
        "---\n",
        "\n",
        "#### Inputs:\n",
        "- **`user_id`**: The ID of the user for whom recommendations are being generated.\n",
        "- **`timestamp`**: The timestamp of the user's current interaction.\n",
        "- **`recommended_articles_ids`**: A list of article IDs recommended by collaborative filtering.\n",
        "- **`features`**: A TF-IDF feature matrix, where rows correspond to articles and columns represent term importance.\n",
        "- **`k`**: The number of top articles to recommend.\n",
        "\n",
        "---\n",
        "\n",
        "#### Steps:\n",
        "1. **Retrieve User History**:\n",
        "   - Fetches the list of articles previously read by the user based on their `user_id` and `timestamp`.\n",
        "\n",
        "2. **Identify Relevant Indices**:\n",
        "   - **For Read Articles**: Finds the indices of the articles in `news_df` that match the IDs in the user's history.\n",
        "   - **For Recommended Articles**: Identifies indices of articles from `recommended_articles_ids` in `news_df`.\n",
        "\n",
        "3. **Filter Unread Articles**:\n",
        "   - Excludes articles already read by the user from the list of potential recommendations.\n",
        "\n",
        "4. **Create a User Profile**:\n",
        "   - Aggregates the TF-IDF feature vectors of articles in the user's history to form a \"user profile\" vector. This is calculated as the average feature vector of the read articles.\n",
        "\n",
        "5. **Calculate Similarity Scores**:\n",
        "   - Computes the cosine similarity between the user profile vector and the feature vectors of the recommended (but unread) articles.\n",
        "\n",
        "6. **Sort and Select Top-k Articles**:\n",
        "   - Sorts the recommended articles by their similarity scores in descending order.\n",
        "   - Extracts the IDs of the top `k` articles with the highest similarity scores.\n",
        "\n",
        "7. **Output the Recommendations**:\n",
        "   - Returns a list of article IDs corresponding to the top `k` recommendations.\n",
        "\n",
        "---\n",
        "\n",
        "#### Outputs:\n",
        "- **`final_recommended_article_ids`**:\n",
        "  A list of the top `k` recommended article IDs based on TF-IDF similarity.\n",
        "\n",
        "---\n",
        "\n",
        "This approach enhances personalized recommendations by aligning the TF-IDF-based content of unread articles with the user’s preferences derived from their reading history.\n"
      ],
      "metadata": {
        "id": "vud4DreaSYpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykRmF59IYNg1"
      },
      "outputs": [],
      "source": [
        "def create_tfidf_features(news_df):\n",
        "  # Create the TF-IDF vectorizer with preprocessing\n",
        "  tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                          lowercase=True,\n",
        "                          tokenizer=word_tokenize,\n",
        "                          use_idf=True,\n",
        "                          norm='l2',\n",
        "                          smooth_idf=True,\n",
        "                          stop_words='english',\n",
        "                          max_df=0.5,\n",
        "                          sublinear_tf=True)\n",
        "\n",
        "  # Fit and transform the combined column\n",
        "  features = tfidf.fit_transform(news_df['Content'])\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_articles_content_tfidf(user_id, timestamp, recommended_article_ids, features, k=5):\n",
        "    \"\"\"\n",
        "    Recommend articles based on TF-IDF similarity.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch previously read article IDs for the user\n",
        "    previously_read_article_ids = (\n",
        "        behaviors_df.loc[\n",
        "            (behaviors_df['User ID'] == user_id) & (behaviors_df['Timestamp'] == timestamp),\n",
        "            'History'\n",
        "        ].iloc[0].split()\n",
        "    )\n",
        "\n",
        "    # Find indices of previously read articles in the feature matrix\n",
        "    user_read_indices = news_df.index[news_df['News ID'].isin(previously_read_article_ids)].tolist()\n",
        "\n",
        "    # Find indices of recommended articles in the feature matrix\n",
        "    recommended_indices = news_df.index[news_df['News ID'].isin(recommended_article_ids)].tolist()\n",
        "\n",
        "    # Ensure recommended indices exclude already read articles\n",
        "    filtered_indices = [idx for idx in recommended_indices if idx not in user_read_indices]\n",
        "\n",
        "    # Aggregate TF-IDF vectors for the articles read by the user\n",
        "    user_profile_vector = features[user_read_indices].mean(axis=0).A1\n",
        "\n",
        "    # Compute cosine similarity between the user profile and recommended articles\n",
        "    similarity_scores = cosine_similarity([user_profile_vector], features[filtered_indices]).flatten()\n",
        "\n",
        "    # Retrieve the indices of the top-k articles based on similarity scores\n",
        "    top_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
        "\n",
        "    # Map back to article IDs and return the top recommendations\n",
        "    top_article_ids = news_df.iloc[np.array(filtered_indices)[top_indices]]['News ID'].tolist()\n",
        "\n",
        "    return top_article_ids\n"
      ],
      "metadata": {
        "id": "WgTWlaorMhVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cARuG1m6uqNv"
      },
      "source": [
        "### Single User"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def single_user_recommendations_combined(user_id, timestamp, method='word2vec', similar_user_k=5, articles_k=5):\n",
        "    \"\"\"\n",
        "    Generate article recommendations for a single user using a chosen method.\n",
        "\n",
        "    Args:\n",
        "    - user_id (str): User ID for whom recommendations are generated.\n",
        "    - timestamp (str): Timestamp of the interaction.\n",
        "    - method (str): Recommendation method ('word2vec', 'embeddings', or 'tfidf').\n",
        "    - similar_user_k (int): Number of similar users to consider.\n",
        "    - articles_k (int): Number of articles to recommend.\n",
        "\n",
        "    Returns:\n",
        "    - list: Recommended article IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch similar users and their interaction timestamps\n",
        "    similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n",
        "\n",
        "    # Identify articles read by similar users\n",
        "    recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n",
        "\n",
        "    # Handle recommendations based on the chosen method\n",
        "    if method == 'word2vec':\n",
        "        # Generate content from previously read and recommended articles\n",
        "        previously_read_content = create_previously_read_content(user_id, timestamp)\n",
        "        recommended_content = create_recommended_content(recommended_article_ids)\n",
        "\n",
        "        # Recommend articles using Word2Vec-based content similarity\n",
        "        final_recommended_article_ids = recommend_articles_content_w2v(\n",
        "            previously_read_content, recommended_content, k=articles_k\n",
        "        )\n",
        "\n",
        "    elif method == 'embeddings':\n",
        "        # Recommend articles using embeddings-based collaborative filtering\n",
        "        final_recommended_article_ids = get_top_k_recommended_article_ids_avgvec(\n",
        "            user_id, timestamp, recommended_article_ids, k=articles_k\n",
        "        )\n",
        "\n",
        "    elif method == 'tfidf':\n",
        "        # Generate TF-IDF features from the news data\n",
        "        features = create_tfidf_features(news_df)\n",
        "\n",
        "        # Recommend articles using TF-IDF-based similarity\n",
        "        final_recommended_article_ids = recommend_articles_content_tfidf(\n",
        "            user_id, timestamp, recommended_article_ids, features, k=articles_k\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "    return final_recommended_article_ids\n"
      ],
      "metadata": {
        "id": "1kFAbfvqJMEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdgvwoQuqNv"
      },
      "source": [
        "### Multiple Users"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_user_recommendations_combined(user_ids_timestamps, method='tfidf', similar_user_k=5, articles_k=5):\n",
        "    \"\"\"\n",
        "    Generate article recommendations for multiple users based on the chosen method.\n",
        "\n",
        "    Args:\n",
        "    - user_ids_timestamps (list): List of tuples containing user IDs and their timestamps.\n",
        "    - method (str): Recommendation method ('word2vec', 'embeddings', or 'tfidf').\n",
        "    - similar_user_k (int): Number of similar users to consider for recommendations.\n",
        "    - articles_k (int): Number of articles to recommend for each user.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary where keys are user IDs and values are lists of recommended article IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize dictionary to store recommendations for each user\n",
        "    user_recommendations_dict = {}\n",
        "\n",
        "    # Precompute TF-IDF features if the chosen method is 'tfidf'\n",
        "    features = None\n",
        "    if method == 'tfidf':\n",
        "        features = create_tfidf_features(news_df)\n",
        "\n",
        "    # Iterate over the list of user IDs and timestamps\n",
        "    for counter, (user_id, timestamp) in enumerate(user_ids_timestamps, start=1):\n",
        "        # Fetch similar users and their interaction timestamps\n",
        "        similar_users_timestamps = fetch_similar_users(user_id, timestamp, k=similar_user_k)\n",
        "\n",
        "        # Identify articles read by similar users\n",
        "        recommended_article_ids = recommend_articles_collaborative(user_id, timestamp, similar_users_timestamps)\n",
        "\n",
        "        if method == 'word2vec':\n",
        "            # Generate content for previously read and recommended articles\n",
        "            previously_read_content = create_previously_read_content(user_id, timestamp)\n",
        "            recommended_content = create_recommended_content(recommended_article_ids)\n",
        "\n",
        "            # Recommend articles using Word2Vec-based content similarity\n",
        "            final_recommended_ids = recommend_articles_content_w2v(\n",
        "                previously_read_content, recommended_content, k=articles_k\n",
        "            )\n",
        "\n",
        "        elif method == 'embeddings':\n",
        "            # Recommend articles using embeddings-based collaborative filtering\n",
        "            final_recommended_ids = get_top_k_recommended_article_ids_avgvec(\n",
        "                user_id, timestamp, recommended_article_ids, k=articles_k\n",
        "            )\n",
        "\n",
        "        elif method == 'tfidf':\n",
        "            # Recommend articles using TF-IDF-based similarity\n",
        "            final_recommended_ids = recommend_articles_content_tfidf(\n",
        "                user_id, timestamp, recommended_article_ids, features, k=articles_k\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Store recommendations for the user in the dictionary\n",
        "        user_recommendations_dict[user_id] = final_recommended_ids\n",
        "\n",
        "        # Optional progress tracking\n",
        "        print(f\"Processed user {counter}/{len(user_ids_timestamps)}\")\n",
        "\n",
        "    return user_recommendations_dict\n"
      ],
      "metadata": {
        "id": "3lHdtvGIG_pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4FmCwhYuqNw"
      },
      "source": [
        "## Test on Sample User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJkO1IPEuqNw"
      },
      "outputs": [],
      "source": [
        " # Run recommender system\n",
        "final_recommended_ids = single_user_recommendations_combined('U13740', '2019-11-13 15:27:40', method='word2vec', similar_user_k=5, articles_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omG4By0ZuqNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115f100b-91c6-4212-fd59-53efc4724fa6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N9674', 'N24691', 'N25635', 'N64273', 'N21547']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# view recommendations - avg vec\n",
        "final_recommended_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBQLjSrBuqNw"
      },
      "source": [
        "## Test on Multiple Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWGIbcoguqNx"
      },
      "outputs": [],
      "source": [
        "# Select a subset of users of size k to test on\n",
        "user_ids_timestamps = select_user_ids_timestamps(k=5)\n",
        "\n",
        "# Run recommender system\n",
        "final_recommended_ids_multiple = multiple_user_recommendations_combined(user_ids_timestamps, method='word2vec', similar_user_k=5, articles_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_recommended_ids_multiple"
      ],
      "metadata": {
        "id": "fQUKyYT5s_J0",
        "outputId": "720ad607-ab6e-498f-9e63-297542bb384c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U21593': ['N42781', 'N59704', 'N30665', 'N16655', 'N46039'],\n",
              " 'U10123': ['N8448', 'N16344', 'N27612', 'N41172', 'N10843'],\n",
              " 'U75630': ['N16384', 'N37304', 'N61352', 'N64305', 'N52236'],\n",
              " 'U44625': ['N10928', 'N4255', 'N58860', 'N63302', 'N11523'],\n",
              " 'U64800': ['N17303', 'N27951', 'N287', 'N44021', 'N35170']}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iGe6OANaSNJn",
        "sFRePjiQXuvB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}